{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from funciones_3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de Aceptación de Vehículos. \n",
    "Usted trabaja en el equipo de Machine Learning de una empresa que comercializa vehículos online, y se le ha pedido que desarrolle un modelo para predecir el grado de aceptabilidad de vehículos por parte de los clientes.\n",
    "Para ello le ha sido dado un conjunto de datos recolectados durante 2 años de operación de la empresa, en donde se reporta el precio de venta, costo de mantenimiento, nu ́mero de puertas, capacidad de pasajeros, tamaño del baíl y seguridad estimada de distinos vehículos, así como el grado de aceptación de estos vehículos por parte de los clientes. La aceptación de un vehículo puede interpretarse como la medida en que este cumple con las necesidades, preferencias y requisitos del cliente y se reporta en cuatro categorías: “inaceptable”, “aceptable”, “buena” y “muy buena”. El con- junto de datos se dividió previamente en uno de entrenamiento (car train.csv), uno de validación (car valid.csv) y uno de testeo (car test.csv).\n",
    "Desarrollar al menos dos modelos predictivos distintos que estimen el nivel de aceptación de un vehículo, en base a sus atributos. Explicar cual de todos los modelos desarrollados se debe poner en producción, teniendo en cuenta que lo que más le interesa a la empresa es identifcar bien los vehiculos que son “inaceptables”, así como los que son “muy buenos”, ya que los primeros representan un cliente insastisfecho, y los segundos son los vehículos que se quieren promocionar más.\n",
    "Describa claramente cua ́l fue la estrategia y el proceso que llevó a cabo para llegar al modelo que enviará a producción (o sea, el que usted considera “el mejor” modelo de todos).\n",
    "\n",
    "NOTA: cuando se dice “al menos dos modelos predictivos distintos” esto quiere decir dos modelos con arquitecuras distintas. Por ejemplo, LDA y regresion logística son dos arquitecturas distintas, porque por su estructura de ecuaciones parametrizan el espacio de clasificación de manera diferente, mientras que regresión logistica y red neuronal con activación de salida sigmoide son arquitecturas de alguna manera iguales, ya que la primera es un caso particular de la segunda (regresión logística es esencialmente una red neuronal sin capas ocultas y activación sigmoide). La idea es que generen una diversidad de modelos, y quedarse con el que mejor capacidad predictiva tiene para ese problema en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (1209, 22)\n",
      "\n",
      "test shape:  (260, 22)\n",
      "\n",
      "validation shape:  (259, 22)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/Users/maxi/Downloads/Actuales/ML/ML-TP3/Data/3 - Evaluación de Aceptación de Vehı́culos/car_train.csv')\n",
    "test = pd.read_csv('/Users/maxi/Downloads/Actuales/ML/ML-TP3/Data/3 - Evaluación de Aceptación de Vehı́culos/car_test.csv')\n",
    "validation = pd.read_csv('/Users/maxi/Downloads/Actuales/ML/ML-TP3/Data/3 - Evaluación de Aceptación de Vehı́culos/car_valid.csv')\n",
    "\n",
    "# one hot encoding\n",
    "train = one_hot_encoder(train, ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "test = one_hot_encoder(test, ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "validation = one_hot_encoder(validation, ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "                                          \n",
    "# target encoding   # unacc:0, acc: 1, good: 2, vgood: 3\n",
    "train['acceptability'] = train['acceptability'].map({'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3})\n",
    "test['acceptability'] = test['acceptability'].map({'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3})\n",
    "validation['acceptability'] = validation['acceptability'].map({'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3})\n",
    "\n",
    "# normalize all data\n",
    "# min_max = get_min_max(train)\n",
    "# train = min_max_normalize(train, min_max)\n",
    "# test = min_max_normalize(test, min_max)\n",
    "# validation = min_max_normalize(validation, min_max)\n",
    "\n",
    "# if any column has a missing value replace with cero\n",
    "# train = train.fillna(0)\n",
    "# test = test.fillna(0)\n",
    "# validation = validation.fillna(0)\n",
    "\n",
    "print(\"train shape: \", train.shape)\n",
    "print(\"\\ntest shape: \", test.shape)\n",
    "print(\"\\nvalidation shape: \", validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "acceptability\n",
      "0    847\n",
      "1    269\n",
      "2     48\n",
      "3     45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify how many samples we have for each class in the training set\n",
    "print(\"train:\")\n",
    "print(train['acceptability'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are unbalanced. As a solution to this problem I try using Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (after Oversampling):\n",
      "acceptability\n",
      "0    847\n",
      "1    269\n",
      "2     96\n",
      "3     90\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Oversampling: duplico todos los datos de las clases 2 y 3 del train\n",
    "class_2 = train[train['acceptability'] == 2]\n",
    "class_3 = train[train['acceptability'] == 3]\n",
    "\n",
    "double_class_2 = pd.concat([class_2], ignore_index=True)\n",
    "double_class_3 = pd.concat([class_3], ignore_index=True)\n",
    "\n",
    "train = pd.concat([train, double_class_2, double_class_3], ignore_index=True)\n",
    "\n",
    "print(\"train (after Oversampling):\")\n",
    "print(train['acceptability'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (1302, 21)\n",
      "y_train shape:  (1302,)\n",
      "X_test shape:  (260, 21)\n",
      "y_test shape:  (260,)\n",
      "X_validation shape:  (259, 21)\n",
      "y_validation shape:  (259,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceptability</th>\n",
       "      <th>buying_high</th>\n",
       "      <th>buying_low</th>\n",
       "      <th>buying_med</th>\n",
       "      <th>buying_vhigh</th>\n",
       "      <th>maint_high</th>\n",
       "      <th>maint_low</th>\n",
       "      <th>maint_med</th>\n",
       "      <th>maint_vhigh</th>\n",
       "      <th>doors_2</th>\n",
       "      <th>doors_3</th>\n",
       "      <th>doors_4</th>\n",
       "      <th>doors_5more</th>\n",
       "      <th>persons_2</th>\n",
       "      <th>persons_4</th>\n",
       "      <th>persons_more</th>\n",
       "      <th>lug_boot_big</th>\n",
       "      <th>lug_boot_med</th>\n",
       "      <th>lug_boot_small</th>\n",
       "      <th>safety_high</th>\n",
       "      <th>safety_low</th>\n",
       "      <th>safety_med</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acceptability  buying_high  buying_low  buying_med  buying_vhigh  \\\n",
       "0              0            0           0           1             0   \n",
       "1              0            0           0           1             0   \n",
       "2              0            0           0           0             1   \n",
       "3              3            0           1           0             0   \n",
       "4              0            0           1           0             0   \n",
       "\n",
       "   maint_high  maint_low  maint_med  maint_vhigh  doors_2  doors_3  doors_4  \\\n",
       "0           1          0          0            0        0        0        0   \n",
       "1           0          0          1            0        0        1        0   \n",
       "2           1          0          0            0        0        0        1   \n",
       "3           1          0          0            0        0        1        0   \n",
       "4           0          0          1            0        1        0        0   \n",
       "\n",
       "   doors_5more  persons_2  persons_4  persons_more  lug_boot_big  \\\n",
       "0            1          0          0             1             0   \n",
       "1            0          1          0             0             1   \n",
       "2            0          0          1             0             0   \n",
       "3            0          0          1             0             1   \n",
       "4            0          0          1             0             1   \n",
       "\n",
       "   lug_boot_med  lug_boot_small  safety_high  safety_low  safety_med  \n",
       "0             0               1            0           1           0  \n",
       "1             0               0            0           1           0  \n",
       "2             0               1            0           0           1  \n",
       "3             0               0            1           0           0  \n",
       "4             0               0            0           1           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train.drop(columns=['acceptability'])\n",
    "y_train = train['acceptability'].values\n",
    "\n",
    "X_test = test.drop(columns=['acceptability'])\n",
    "y_test = test['acceptability'].values\n",
    "\n",
    "X_validation = validation.drop(columns=['acceptability'])\n",
    "y_validation = validation['acceptability'].values\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "print(\"X_validation shape: \", X_validation.shape)\n",
    "print(\"y_validation shape: \", y_validation.shape)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "train.head()\n",
    "test.head()\n",
    "validation.head()\n",
    "# X_train.head()\n",
    "# y_train\n",
    "\n",
    "# # show all rows\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "Prueblo de aplicar un modelo de KNN, este modelo probablemente sea inferior al otro, pero sirve como baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8108108108108109\n",
      "F1 Score:  0.6588235294117647\n",
      "Precision:  0.717948717948718\n",
      "Recall:  0.6086956521739131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 170, 11, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNN(k=3)\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred = knn_model.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred))\n",
    "print(\"F1 Score: \", f1(y_validation, y_pred))\n",
    "print(\"Precision: \", precision(y_validation, y_pred))\n",
    "print(\"Recall: \", recall(y_validation, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_validation, y_pred)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árbol de Decisión\n",
    "Pruebo de aplicar un Árbol de decisión. Nuevamente no pretendo que este sea el mejor modelo, sino que lo implemento para ver como funciona y que resultados se logran con un árbol solo antes de aprovechar el uso de métodos de ensamble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9845559845559846\n",
      "Precision:  0.9649122807017544\n",
      "Recall:  1.0\n",
      "F1 Score:  0.9821428571428572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55, 179, 2, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model = DecisionTree(max_depth=10)\n",
    "dt_model.fit(X_train.values, y_train)\n",
    "y_pred = dt_model.predict(X_validation.values)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred))\n",
    "print(\"Precision: \", precision(y_validation, y_pred))\n",
    "print(\"Recall: \", recall(y_validation, y_pred))\n",
    "print(\"F1 Score: \", f1(y_validation, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_validation, y_pred)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Pruebo de aplicar un random forest. Espero que este sea el modelo que de los mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9806949806949807\n",
      "Precision:  0.9642857142857143\n",
      "Recall:  0.9818181818181818\n",
      "F1 Score:  0.972972972972973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(54, 179, 2, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForest(n_trees=100, max_depth=10)\n",
    "rf_model.fit(X_train.values, y_train)\n",
    "y_pred = rf_model.predict(X_validation.values)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred))\n",
    "print(\"Precision: \", precision(y_validation, y_pred))\n",
    "print(\"Recall: \", recall(y_validation, y_pred))\n",
    "print(\"F1 Score: \", f1(y_validation, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_validation, y_pred)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Accuracy:  0.8346153846153846\n",
      "Precision:  0.7804878048780488\n",
      "Recall:  0.64\n",
      "F1 Score:  0.7032967032967035\n",
      "confusion matrix: (32, 170, 9, 18)\n",
      "\n",
      "Decision Tree\n",
      "Accuracy:  0.9807692307692307\n",
      "Precision:  0.9354838709677419\n",
      "Recall:  1.0\n",
      "F1 Score:  0.9666666666666666\n",
      "confusion matrix: (58, 178, 4, 0)\n"
     ]
    }
   ],
   "source": [
    "# results on test set\n",
    "y_pred = knn_model.predict(X_test)\n",
    "print(\"KNN\")\n",
    "print(\"Accuracy: \", accuracy(y_test, y_pred))\n",
    "print(\"Precision: \", precision(y_test, y_pred))\n",
    "print(\"Recall: \", recall(y_test, y_pred))\n",
    "print(\"F1 Score: \", f1(y_test, y_pred))\n",
    "print(f\"confusion matrix: {confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "y_pred = dt_model.predict(X_test.values)\n",
    "print(\"\\nDecision Tree\")\n",
    "print(\"Accuracy: \", accuracy(y_test, y_pred))\n",
    "print(\"Precision: \", precision(y_test, y_pred))\n",
    "print(\"Recall: \", recall(y_test, y_pred))\n",
    "print(\"F1 Score: \", f1(y_test, y_pred))\n",
    "print(f\"confusion matrix: {confusion_matrix(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando RandomForest estoy obteniendo buenos resultados. Hago cross-validation para encontrar los mejores hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [5, 8, 10, 12, 15]\n",
    "forest_sizes = [10, 50, 100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 59.21it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 60.92it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 62.90it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 61.70it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 61.74it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 52.84it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 63.04it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 65.18it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 59.88it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 60.41it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 63.58it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 63.47it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 61.40it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 63.44it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 64.79it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 64.23it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 63.94it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 64.27it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 61.53it/s]\n",
      "100%|██████████| 50/50 [00:00<00:00, 58.98it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 62.21it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 64.17it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 64.47it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 64.48it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 62.94it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 63.98it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 63.59it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 62.42it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 62.88it/s]\n",
      "100%|██████████| 100/100 [00:01<00:00, 63.63it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 64.01it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 63.39it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 63.50it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 61.33it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 62.82it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 62.91it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 63.00it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 63.07it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 62.72it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 63.27it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 62.96it/s]\n",
      "100%|██████████| 500/500 [00:08<00:00, 62.17it/s]\n",
      "100%|██████████| 500/500 [00:08<00:00, 62.47it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 62.79it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 63.59it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 63.41it/s]\n",
      "100%|██████████| 500/500 [00:08<00:00, 61.45it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 62.87it/s]\n",
      "100%|██████████| 500/500 [00:08<00:00, 62.45it/s]\n",
      "100%|██████████| 500/500 [00:07<00:00, 63.71it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 23.47it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.31it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.12it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 21.93it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.66it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.19it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 22.09it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 20.47it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.72it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.27it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.66it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.78it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.45it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 22.59it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 20.92it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.73it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 21.50it/s]\n",
      "100%|██████████| 50/50 [00:02<00:00, 19.01it/s]\n",
      "100%|██████████| 100/100 [00:05<00:00, 18.78it/s]\n",
      "100%|██████████| 100/100 [00:05<00:00, 19.53it/s]\n",
      "100%|██████████| 100/100 [00:05<00:00, 19.51it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 21.03it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 21.00it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.19it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 22.13it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.88it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.56it/s]\n",
      "100%|██████████| 100/100 [00:04<00:00, 20.94it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.56it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.54it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.19it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.35it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.53it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.18it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 21.47it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.52it/s]\n",
      "100%|██████████| 200/200 [00:10<00:00, 19.77it/s]\n",
      "100%|██████████| 200/200 [00:09<00:00, 20.61it/s]\n",
      "100%|██████████| 500/500 [00:24<00:00, 20.22it/s]\n",
      "100%|██████████| 500/500 [00:24<00:00, 20.76it/s]\n",
      "100%|██████████| 500/500 [00:25<00:00, 19.82it/s]\n",
      "100%|██████████| 500/500 [00:23<00:00, 21.55it/s]\n",
      "100%|██████████| 500/500 [00:23<00:00, 20.84it/s]\n",
      "100%|██████████| 500/500 [00:23<00:00, 21.34it/s]\n",
      "100%|██████████| 500/500 [00:24<00:00, 20.40it/s]\n",
      "100%|██████████| 500/500 [00:23<00:00, 20.94it/s]\n",
      "100%|██████████| 500/500 [00:24<00:00, 20.55it/s]\n",
      "100%|██████████| 500/500 [00:23<00:00, 21.09it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.77it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.96it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.21it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.28it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 15.29it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.97it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.56it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.74it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.98it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 15.17it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.62it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.87it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.40it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.59it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.27it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.06it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.77it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.64it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 14.60it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 14.15it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 14.64it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.10it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 14.06it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 14.48it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 14.70it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 14.12it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 14.55it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 14.18it/s]\n",
      "100%|██████████| 100/100 [00:06<00:00, 15.30it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.41it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.84it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.60it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.42it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.86it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.50it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.50it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.39it/s]\n",
      "100%|██████████| 200/200 [00:14<00:00, 14.24it/s]\n",
      "100%|██████████| 200/200 [00:13<00:00, 14.58it/s]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.62it/s]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.67it/s]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.69it/s]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.67it/s]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.42it/s]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.88it/s]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.49it/s]\n",
      "100%|██████████| 500/500 [00:33<00:00, 14.86it/s]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.34it/s]\n",
      "100%|██████████| 500/500 [00:34<00:00, 14.64it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.58it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.95it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.47it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.56it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.92it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.95it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.66it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.49it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.26it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.71it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.79it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.58it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.24it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.59it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.52it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.39it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.54it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 12.55it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.14it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.25it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.68it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.71it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.81it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.28it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.48it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.67it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.87it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.28it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.08it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.75it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.51it/s]\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.42it/s]\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.68it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.91it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.04it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.98it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.20it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.14it/s]\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.36it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.13it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.00it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.10it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.75it/s]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.17it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.09it/s]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.20it/s]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.27it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.90it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.08it/s]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.56it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.24it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.43it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.66it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 10.44it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.13it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.17it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.19it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.20it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.41it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 12.75it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.04it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.16it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.58it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.56it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.30it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 12.84it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 12.55it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.26it/s]\n",
      "100%|██████████| 50/50 [00:03<00:00, 13.23it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.28it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.24it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.99it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.91it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.89it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.35it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.26it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.88it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 12.81it/s]\n",
      "100%|██████████| 100/100 [00:07<00:00, 13.61it/s]\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.46it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.20it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.23it/s]\n",
      "100%|██████████| 200/200 [00:14<00:00, 13.46it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.28it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.16it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.18it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.95it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.07it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.19it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.97it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.15it/s]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.24it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 12.91it/s]\n",
      "100%|██████████| 500/500 [00:38<00:00, 13.07it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.77it/s]\n",
      "100%|██████████| 500/500 [00:37<00:00, 13.30it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.67it/s]\n",
      "100%|██████████| 500/500 [00:36<00:00, 13.71it/s]\n",
      "100%|██████████| 500/500 [00:39<00:00, 12.80it/s]\n"
     ]
    }
   ],
   "source": [
    "acc_train, acc_test = cross_validation(X_train, y_train, max_depths, forest_sizes, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: (12, 500)\n",
      "Best average test accuracy: 0.99\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "best_hparams = None\n",
    "\n",
    "for i, depth in enumerate(max_depths):\n",
    "    for j, forest_size in enumerate(forest_sizes):\n",
    "        avg_train_accuracy = np.mean(acc_train[i * len(forest_sizes) + j])\n",
    "        avg_test_accuracy = np.mean(acc_test[i * len(forest_sizes) + j])\n",
    "        \n",
    "        if avg_test_accuracy > best_accuracy:\n",
    "            best_accuracy = avg_test_accuracy\n",
    "            best_hparams = (depth, forest_size)\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hparams)\n",
    "print(\"Best average test accuracy:\", best_accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:39<00:00, 12.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set\n",
      "Accuracy:  0.9806949806949807\n",
      "Precision:  0.9642857142857143\n",
      "Recall:  0.9818181818181818\n",
      "F1 Score:  0.972972972972973\n",
      "confusion matrix: (54, 179, 2, 1)\n",
      "\n",
      "Test set\n",
      "Accuracy:  0.9846153846153847\n",
      "Precision:  0.9655172413793104\n",
      "Recall:  0.9655172413793104\n",
      "F1 Score:  0.9655172413793104\n",
      "confusion matrix: (56, 180, 2, 2)\n"
     ]
    }
   ],
   "source": [
    "# train model with best hyperparameters\n",
    "rf_model = RandomForest(n_trees=best_hparams[1], max_depth=best_hparams[0])\n",
    "rf_model.fit(X_train.values, y_train)\n",
    "y_pred = rf_model.predict(X_validation.values)\n",
    "y_pred_test = rf_model.predict(X_test.values)\n",
    "\n",
    "print(\"Validation set\")\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred))\n",
    "print(\"Precision: \", precision(y_validation, y_pred))\n",
    "print(\"Recall: \", recall(y_validation, y_pred))\n",
    "print(\"F1 Score: \", f1(y_validation, y_pred))\n",
    "print(f\"confusion matrix: {confusion_matrix(y_validation, y_pred)}\")\n",
    "\n",
    "print(\"\\nTest set\")\n",
    "print(\"Accuracy: \", accuracy(y_test, y_pred_test))\n",
    "print(\"Precision: \", precision(y_test, y_pred_test))\n",
    "print(\"Recall: \", recall(y_test, y_pred_test))\n",
    "print(\"F1 Score: \", f1(y_test, y_pred_test))\n",
    "print(f\"confusion matrix: {confusion_matrix(y_test, y_pred_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
