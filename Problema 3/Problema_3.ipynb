{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from funciones_3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de Aceptación de Vehículos. \n",
    "Usted trabaja en el equipo de Machine Learning de una empresa que comercializa vehículos online, y se le ha pedido que desarrolle un modelo para predecir el grado de aceptabilidad de vehículos por parte de los clientes.\n",
    "Para ello le ha sido dado un conjunto de datos recolectados durante 2 años de operación de la empresa, en donde se reporta el precio de venta, costo de mantenimiento, nu ́mero de puertas, capacidad de pasajeros, tamaño del baíl y seguridad estimada de distinos vehículos, así como el grado de aceptación de estos vehículos por parte de los clientes. La aceptación de un vehículo puede interpretarse como la medida en que este cumple con las necesidades, preferencias y requisitos del cliente y se reporta en cuatro categorías: “inaceptable”, “aceptable”, “buena” y “muy buena”. El con- junto de datos se dividió previamente en uno de entrenamiento (car train.csv), uno de validación (car valid.csv) y uno de testeo (car test.csv).\n",
    "Desarrollar al menos dos modelos predictivos distintos que estimen el nivel de aceptación de un vehículo, en base a sus atributos. Explicar cual de todos los modelos desarrollados se debe poner en producción, teniendo en cuenta que lo que más le interesa a la empresa es identifcar bien los vehiculos que son “inaceptables”, así como los que son “muy buenos”, ya que los primeros representan un cliente insastisfecho, y los segundos son los vehículos que se quieren promocionar más.\n",
    "Describa claramente cua ́l fue la estrategia y el proceso que llevó a cabo para llegar al modelo que enviará a producción (o sea, el que usted considera “el mejor” modelo de todos).\n",
    "\n",
    "NOTA: cuando se dice “al menos dos modelos predictivos distintos” esto quiere decir dos modelos con arquitecuras distintas. Por ejemplo, LDA y regresion logística son dos arquitecturas distintas, porque por su estructura de ecuaciones parametrizan el espacio de clasificación de manera diferente, mientras que regresión logistica y red neuronal con activación de salida sigmoide son arquitecturas de alguna manera iguales, ya que la primera es un caso particular de la segunda (regresión logística es esencialmente una red neuronal sin capas ocultas y activación sigmoide). La idea es que generen una diversidad de modelos, y quedarse con el que mejor capacidad predictiva tiene para ese problema en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (1209, 22)\n",
      "\n",
      "test shape:  (260, 22)\n",
      "\n",
      "validation shape:  (259, 22)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/Users/maxi/Downloads/Actuales/ML/ML-TP3/Data/3 - Evaluación de Aceptación de Vehı́culos/car_train.csv')\n",
    "test = pd.read_csv('/Users/maxi/Downloads/Actuales/ML/ML-TP3/Data/3 - Evaluación de Aceptación de Vehı́culos/car_test.csv')\n",
    "validation = pd.read_csv('/Users/maxi/Downloads/Actuales/ML/ML-TP3/Data/3 - Evaluación de Aceptación de Vehı́culos/car_valid.csv')\n",
    "\n",
    "# one hot encoding\n",
    "train = one_hot_encoder(train, ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "test = one_hot_encoder(test, ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "validation = one_hot_encoder(validation, ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "                                          \n",
    "# target encoding   # unacc:0, acc: 1, good: 2, vgood: 3\n",
    "train['acceptability'] = train['acceptability'].map({'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3})\n",
    "test['acceptability'] = test['acceptability'].map({'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3})\n",
    "validation['acceptability'] = validation['acceptability'].map({'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3})\n",
    "\n",
    "# normalize all data\n",
    "# min_max = get_min_max(train)\n",
    "# train = min_max_normalize(train, min_max)\n",
    "# test = min_max_normalize(test, min_max)\n",
    "# validation = min_max_normalize(validation, min_max)\n",
    "\n",
    "# if any column has a missing value replace with cero\n",
    "# train = train.fillna(0)\n",
    "# test = test.fillna(0)\n",
    "# validation = validation.fillna(0)\n",
    "\n",
    "print(\"train shape: \", train.shape)\n",
    "print(\"\\ntest shape: \", test.shape)\n",
    "print(\"\\nvalidation shape: \", validation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el bloque anterior se bajó la información del dataset. Como todas las variables eran categóricas se pasaron a valores númericos y se realizó one hot encoding para armar un feature con cada clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "acceptability\n",
      "0    847\n",
      "1    269\n",
      "2     48\n",
      "3     45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify how many samples we have for each class in the training set\n",
    "print(\"train:\")\n",
    "print(train['acceptability'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las clases están desbalanceadas y tengo interés por lograr clasificar específicamente bien los datos de las clases 0 y 3, pero la 3 es una clase minoritaria. Para mejorar la precisión de mis modelos uso la estrategia de  Oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (after Oversampling):\n",
      "acceptability\n",
      "0    847\n",
      "1    269\n",
      "2     96\n",
      "3     90\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Oversampling: duplico todos los datos de las clases 2 y 3 del train\n",
    "class_2 = train[train['acceptability'] == 2]\n",
    "class_3 = train[train['acceptability'] == 3]\n",
    "\n",
    "double_class_2 = pd.concat([class_2], ignore_index=True)\n",
    "double_class_3 = pd.concat([class_3], ignore_index=True)\n",
    "\n",
    "train = pd.concat([train, double_class_2, double_class_3], ignore_index=True)\n",
    "\n",
    "print(\"train (after Oversampling):\")\n",
    "print(train['acceptability'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora el train dataset cuenta con más datos de las clases minoritarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (1302, 21)\n",
      "y_train shape:  (1302,)\n",
      "X_test shape:  (260, 21)\n",
      "y_test shape:  (260,)\n",
      "X_validation shape:  (259, 21)\n",
      "y_validation shape:  (259,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceptability</th>\n",
       "      <th>buying_high</th>\n",
       "      <th>buying_low</th>\n",
       "      <th>buying_med</th>\n",
       "      <th>buying_vhigh</th>\n",
       "      <th>maint_high</th>\n",
       "      <th>maint_low</th>\n",
       "      <th>maint_med</th>\n",
       "      <th>maint_vhigh</th>\n",
       "      <th>doors_2</th>\n",
       "      <th>doors_3</th>\n",
       "      <th>doors_4</th>\n",
       "      <th>doors_5more</th>\n",
       "      <th>persons_2</th>\n",
       "      <th>persons_4</th>\n",
       "      <th>persons_more</th>\n",
       "      <th>lug_boot_big</th>\n",
       "      <th>lug_boot_med</th>\n",
       "      <th>lug_boot_small</th>\n",
       "      <th>safety_high</th>\n",
       "      <th>safety_low</th>\n",
       "      <th>safety_med</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acceptability  buying_high  buying_low  buying_med  buying_vhigh  \\\n",
       "0              0            0           0           1             0   \n",
       "1              0            0           0           1             0   \n",
       "2              0            0           0           0             1   \n",
       "3              3            0           1           0             0   \n",
       "4              0            0           1           0             0   \n",
       "\n",
       "   maint_high  maint_low  maint_med  maint_vhigh  doors_2  doors_3  doors_4  \\\n",
       "0           1          0          0            0        0        0        0   \n",
       "1           0          0          1            0        0        1        0   \n",
       "2           1          0          0            0        0        0        1   \n",
       "3           1          0          0            0        0        1        0   \n",
       "4           0          0          1            0        1        0        0   \n",
       "\n",
       "   doors_5more  persons_2  persons_4  persons_more  lug_boot_big  \\\n",
       "0            1          0          0             1             0   \n",
       "1            0          1          0             0             1   \n",
       "2            0          0          1             0             0   \n",
       "3            0          0          1             0             1   \n",
       "4            0          0          1             0             1   \n",
       "\n",
       "   lug_boot_med  lug_boot_small  safety_high  safety_low  safety_med  \n",
       "0             0               1            0           1           0  \n",
       "1             0               0            0           1           0  \n",
       "2             0               1            0           0           1  \n",
       "3             0               0            1           0           0  \n",
       "4             0               0            0           1           0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train.drop(columns=['acceptability'])\n",
    "y_train = train['acceptability'].values\n",
    "\n",
    "X_test = test.drop(columns=['acceptability'])\n",
    "y_test = test['acceptability'].values\n",
    "\n",
    "X_validation = validation.drop(columns=['acceptability'])\n",
    "y_validation = validation['acceptability'].values\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "print(\"X_validation shape: \", X_validation.shape)\n",
    "print(\"y_validation shape: \", y_validation.shape)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "train.head()\n",
    "test.head()\n",
    "validation.head()\n",
    "# X_train.head()\n",
    "# y_train\n",
    "\n",
    "# # show all rows\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN\n",
    "Prueblo de aplicar un modelo de KNN, este modelo probablemente sea inferior al otro, pero sirve como baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8108108108108109\n",
      "Accuracy class 0:  0.9392265193370166\n",
      "Accuracy class 3:  0.5\n"
     ]
    }
   ],
   "source": [
    "knn_model = KNN(k=3)\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred = knn_model.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred))\n",
    "# print(\"F1 Score: \", f1(y_validation, y_pred))\n",
    "# print(\"Precision: \", precision(y_validation, y_pred))\n",
    "# print(\"Recall: \", recall(y_validation, y_pred))\n",
    "\n",
    "print(\"Accuracy class 0: \", acc_class_0(y_validation, y_pred))\n",
    "print(\"Accuracy class 3: \", acc_class_3(y_validation, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_validation, y_pred)\n",
    "# conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede observar que el accuracy general es decente, el accuracy de la clase 0 es el más alto, pero el accuracy para la clase 3 es de un 50%, lo cual es inaceptable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árbol de Decisión\n",
    "Pruebo de aplicar un Árbol de decisión. Nuevamente no pretendo que este sea el mejor modelo, sino que lo implemento para ver como funciona y que resultados se logran con un árbol solo antes de aprovechar el uso de métodos de ensamble.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9845559845559846\n",
      "Accuracy class 0:  0.988950276243094\n",
      "Accuracy class 3:  1.0\n"
     ]
    }
   ],
   "source": [
    "dt_model = DecisionTree(max_depth=10)\n",
    "dt_model.fit(X_train.values, y_train)\n",
    "y_pred = dt_model.predict(X_validation.values)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred))\n",
    "# print(\"Precision: \", precision(y_validation, y_pred))\n",
    "# print(\"Recall: \", recall(y_validation, y_pred))\n",
    "# print(\"F1 Score: \", f1(y_validation, y_pred))\n",
    "\n",
    "print(\"Accuracy class 0: \", acc_class_0(y_validation, y_pred))\n",
    "print(\"Accuracy class 3: \", acc_class_3(y_validation, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_validation, y_pred)\n",
    "# conf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso todos los accuracies calculados son muy altos y parecería un modelo aceptable para poner en producción. Pero antes hay que hacer mas pruebas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_validation:\n",
      "0    181\n",
      "1     57\n",
      "2     11\n",
      "3     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "y_pred:\n",
      "0    190\n",
      "1     42\n",
      "2     16\n",
      "3     11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# # Imprimo df comparando y_pred con y_validation\n",
    "# df_comp = pd.DataFrame({'y_validation': y_validation, 'y_pred': y_pred})\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# df_comp\n",
    "\n",
    "# # imprimo cantidad de cada clase en y_validation\n",
    "print(\"y_validation:\")\n",
    "print(pd.Series(y_validation).value_counts())\n",
    "\n",
    "# # imprimo cantidad de cada clase en y_pred\n",
    "print(\"\\ny_pred:\")\n",
    "print(pd.Series(y_pred).value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "Pruebo de aplicar un random forest. Espero que este sea el modelo que de los mejores resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:07<00:00, 13.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9806949806949807\n",
      "Accuracy class 0:  0.988950276243094\n",
      "Accuracy class 3:  1.0\n"
     ]
    }
   ],
   "source": [
    "rf_model = RandomForest(n_trees=100, max_depth=10)\n",
    "rf_model.fit(X_train.values, y_train)\n",
    "y_pred_rf = rf_model.predict(X_validation.values)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred_rf))\n",
    "# print(\"Precision: \", precision(y_validation, y_pred_rf))\n",
    "# print(\"Recall: \", recall(y_validation, y_pred_rf))\n",
    "# print(\"F1 Score: \", f1(y_validation, y_pred_rf))\n",
    "\n",
    "print(\"Accuracy class 0: \", acc_class_0(y_validation, y_pred_rf))\n",
    "print(\"Accuracy class 3: \", acc_class_3(y_validation, y_pred_rf))\n",
    "\n",
    "# confusion matrix\n",
    "# conf_matrix = confusion_matrix(y_validation, y_pred_rf)\n",
    "# conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_validation:\n",
      "0    181\n",
      "1     57\n",
      "2     11\n",
      "3     10\n",
      "Name: count, dtype: int64\n",
      "\n",
      "y_pred:\n",
      "0    180\n",
      "1     56\n",
      "2     12\n",
      "3     11\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print(\"y_pred vs y_validation\")\n",
    "# # armo df para comparar y_pred vs y_validation\n",
    "# df_comp = pd.DataFrame({'y_validation': y_validation, 'y_pred': y_pred})\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# df_comp\n",
    "\n",
    "# # imprimo cantidad de cada clase en y_validation\n",
    "print(\"y_validation:\")\n",
    "print(pd.Series(y_validation).value_counts())\n",
    "\n",
    "# # imprimo cantidad de cada clase en y_pred\n",
    "print(\"\\ny_pred:\")\n",
    "print(pd.Series(y_pred_rf).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso se puede observar que las predicciones, mejoraron un poco respecto de las anteriores y son casi perfectas (en el conjunto de validación)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Accuracy:  0.8346153846153846\n",
      "Accuracy class 0:  0.9340659340659341\n",
      "Accuracy class 3:  0.8\n",
      "\n",
      "Decision Tree\n",
      "Accuracy:  0.9807692307692307\n",
      "Accuracy class 0:  0.978021978021978\n",
      "Accuracy class 3:  1.0\n",
      "\n",
      "Random Forest\n",
      "Accuracy:  0.9769230769230769\n",
      "Accuracy class 0:  0.9835164835164835\n",
      "Accuracy class 3:  1.0\n"
     ]
    }
   ],
   "source": [
    "# results on test set\n",
    "y_pred = knn_model.predict(X_test)\n",
    "print(\"KNN\")\n",
    "print(\"Accuracy: \", accuracy(y_test, y_pred))\n",
    "print(\"Accuracy class 0: \", acc_class_0(y_test, y_pred))\n",
    "print(\"Accuracy class 3: \", acc_class_3(y_test, y_pred))\n",
    "# print(\"Precision: \", precision(y_test, y_pred))\n",
    "# print(\"Recall: \", recall(y_test, y_pred))\n",
    "# print(\"F1 Score: \", f1(y_test, y_pred))\n",
    "# print(f\"confusion matrix: {confusion_matrix(y_test, y_pred)}\") # creo que esto no sirve porque es multiclase\n",
    "\n",
    "y_pred = dt_model.predict(X_test.values)\n",
    "print(\"\\nDecision Tree\")\n",
    "print(\"Accuracy: \", accuracy(y_test, y_pred))\n",
    "print(\"Accuracy class 0: \", acc_class_0(y_test, y_pred))\n",
    "print(\"Accuracy class 3: \", acc_class_3(y_test, y_pred))\n",
    "# print(\"Precision: \", precision(y_test, y_pred))\n",
    "# print(\"Recall: \", recall(y_test, y_pred))\n",
    "# print(\"F1 Score: \", f1(y_test, y_pred))\n",
    "# print(f\"confusion matrix: {confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "y_pred = rf_model.predict(X_test.values)\n",
    "print(\"\\nRandom Forest\")\n",
    "print(\"Accuracy: \", accuracy(y_test, y_pred))\n",
    "print(\"Accuracy class 0: \", acc_class_0(y_test, y_pred))\n",
    "print(\"Accuracy class 3: \", acc_class_3(y_test, y_pred))\n",
    "# print(\"Precision: \", precision(y_test, y_pred))\n",
    "# print(\"Recall: \", recall(y_test, y_pred))\n",
    "# print(\"F1 Score: \", f1(y_test, y_pred))\n",
    "# print(f\"confusion matrix: {confusion_matrix(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando RandomForest se están obteniendo excelentes resultados. Realizó cross-validation para encontrar los mejores hiperparámetros para este modelo. Para definir que hiperparámetros son los óptimos tomo como métrica el promedio entre el accuracy de las predicciones de clase 0 y 3, que fueron definidas como las clases mas importantes de identificar correctamente.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_depths = [5, 8, 10, 12, 15]\n",
    "# forest_sizes = [10, 50, 100, 200]\n",
    "\n",
    "max_depths = [5, 15]\n",
    "forest_sizes = [10, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 57.25it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 61.21it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 63.66it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 64.60it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 64.57it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 64.27it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 62.81it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 65.32it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 62.94it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 63.92it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 62.44it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 64.55it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 62.65it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 64.45it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 65.24it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 65.80it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 60.83it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 64.08it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 63.55it/s]\n",
      "100%|██████████| 200/200 [00:03<00:00, 64.50it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.62it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.84it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 14.24it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.88it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.56it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 11.83it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.10it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 12.78it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.50it/s]\n",
      "100%|██████████| 10/10 [00:00<00:00, 13.14it/s]\n",
      "100%|██████████| 200/200 [00:16<00:00, 12.13it/s]\n",
      "100%|██████████| 200/200 [00:16<00:00, 12.33it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.72it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.81it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.56it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.76it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.55it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.65it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 12.61it/s]\n",
      "100%|██████████| 200/200 [00:15<00:00, 13.06it/s]\n"
     ]
    }
   ],
   "source": [
    "acc_train, acc_test, acc_0, acc_3 = cross_validation(X_train, y_train, max_depths, forest_sizes, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: (15, 10)\n",
      "Best average test accuracy: 0.9859211152932554\n",
      "Best average test accuracy (0): 0.9832291613484287\n",
      "Best average test accuracy (3): 0.97488344988345\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "best_hparams = None\n",
    "\n",
    "for i, depth in enumerate(max_depths):\n",
    "    for j, forest_size in enumerate(forest_sizes):\n",
    "        avg_train_accuracy = np.mean(acc_train[i * len(forest_sizes) + j])\n",
    "        avg_test_accuracy = np.mean(acc_test[i * len(forest_sizes) + j])\n",
    "\n",
    "        avg_train_acc_0 = np.mean(acc_0[i * len(forest_sizes) + j])\n",
    "        avg_train_acc_3 = np.mean(acc_3[i * len(forest_sizes) + j])\n",
    "        avg_test_acc_0 = np.mean(acc_0[i * len(forest_sizes) + j])\n",
    "        avg_test_acc_3 = np.mean(acc_3[i * len(forest_sizes) + j])\n",
    "\n",
    "        avg_train_top_classes_acc = (avg_train_acc_0 + avg_train_acc_3) / 2\n",
    "        avg_test_top_classes_acc = (avg_test_acc_0 + avg_test_acc_3) / 2\n",
    "        \n",
    "        # choose the best hyperparameters based on the average accuracy of the top classes (0 and 3)\n",
    "        if avg_test_top_classes_acc > best_accuracy:\n",
    "            best_accuracy = avg_test_top_classes_acc\n",
    "            best_hparams = (depth, forest_size)\n",
    "\n",
    "print(\"Best hyperparameters:\", best_hparams)\n",
    "print(\"Best average test accuracy:\", best_accuracy)\n",
    "print(\"Best average test accuracy (0):\", avg_test_acc_0)\n",
    "print(\"Best average test accuracy (3):\", avg_test_acc_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 12.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set\n",
      "Accuracy:  0.9536679536679536\n",
      "Accuracy class 0:  0.9723756906077348\n",
      "Accuracy class 3:  0.9\n",
      "\n",
      "Test set\n",
      "Accuracy:  0.9730769230769231\n",
      "Accuracy class 0:  0.989010989010989\n",
      "Accuracy class 3:  0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train model with best hyperparameters\n",
    "rf_model = RandomForest(n_trees=best_hparams[1], max_depth=best_hparams[0])\n",
    "rf_model.fit(X_train.values, y_train)\n",
    "y_pred = rf_model.predict(X_validation.values)\n",
    "y_pred_test = rf_model.predict(X_test.values)\n",
    "\n",
    "print(\"Validation set\")\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred))\n",
    "print(\"Accuracy class 0: \", acc_class_0(y_validation, y_pred))\n",
    "print(\"Accuracy class 3: \", acc_class_3(y_validation, y_pred))\n",
    "# print(\"Precision: \", precision(y_validation, y_pred))\n",
    "# print(\"Recall: \", recall(y_validation, y_pred))\n",
    "# print(\"F1 Score: \", f1(y_validation, y_pred))\n",
    "# print(f\"confusion matrix: {confusion_matrix(y_validation, y_pred)}\")\n",
    "\n",
    "print(\"\\nTest set\")\n",
    "print(\"Accuracy: \", accuracy(y_test, y_pred_test))\n",
    "print(\"Accuracy class 0: \", acc_class_0(y_test, y_pred_test))\n",
    "print(\"Accuracy class 3: \", acc_class_3(y_test, y_pred_test))\n",
    "# print(\"Precision: \", precision(y_test, y_pred_test))\n",
    "# print(\"Recall: \", recall(y_test, y_pred_test))\n",
    "# print(\"F1 Score: \", f1(y_test, y_pred_test))\n",
    "# print(f\"confusion matrix: {confusion_matrix(y_test, y_pred_test)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
