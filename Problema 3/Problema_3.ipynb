{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from funciones_3 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación de Aceptación de Vehículos. \n",
    "Usted trabaja en el equipo de Machine Learning de una empresa que comercializa vehículos online, y se le ha pedido que desarrolle un modelo para predecir el grado de aceptabilidad de vehículos por parte de los clientes.\n",
    "Para ello le ha sido dado un conjunto de datos recolectados durante 2 años de operación de la empresa, en donde se reporta el precio de venta, costo de mantenimiento, nu ́mero de puertas, capacidad de pasajeros, tamaño del baíl y seguridad estimada de distinos vehículos, así como el grado de aceptación de estos vehículos por parte de los clientes. La aceptación de un vehículo puede interpretarse como la medida en que este cumple con las necesidades, preferencias y requisitos del cliente y se reporta en cuatro categorías: “inaceptable”, “aceptable”, “buena” y “muy buena”. El con- junto de datos se dividió previamente en uno de entrenamiento (car train.csv), uno de validación (car valid.csv) y uno de testeo (car test.csv).\n",
    "Desarrollar al menos dos modelos predictivos distintos que estimen el nivel de aceptación de un vehículo, en base a sus atributos. Explicar cual de todos los modelos desarrollados se debe poner en producción, teniendo en cuenta que lo que más le interesa a la empresa es identifcar bien los vehiculos que son “inaceptables”, así como los que son “muy buenos”, ya que los primeros representan un cliente insastisfecho, y los segundos son los vehículos que se quieren promocionar más.\n",
    "Describa claramente cua ́l fue la estrategia y el proceso que llevó a cabo para llegar al modelo que enviará a producción (o sea, el que usted considera “el mejor” modelo de todos).\n",
    "\n",
    "NOTA: cuando se dice “al menos dos modelos predictivos distintos” esto quiere decir dos modelos con arquitecuras distintas. Por ejemplo, LDA y regresion logística son dos arquitecturas distintas, porque por su estructura de ecuaciones parametrizan el espacio de clasificación de manera diferente, mientras que regresión logistica y red neuronal con activación de salida sigmoide son arquitecturas de alguna manera iguales, ya que la primera es un caso particular de la segunda (regresión logística es esencialmente una red neuronal sin capas ocultas y activación sigmoide). La idea es que generen una diversidad de modelos, y quedarse con el que mejor capacidad predictiva tiene para ese problema en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train shape:  (1209, 22)\n",
      "\n",
      "test shape:  (260, 22)\n",
      "\n",
      "validation shape:  (259, 22)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('/Users/maxi/Downloads/Actuales/ML/ML-TP3/Data/3 - Evaluación de Aceptación de Vehı́culos/car_train.csv')\n",
    "test = pd.read_csv('/Users/maxi/Downloads/Actuales/ML/ML-TP3/Data/3 - Evaluación de Aceptación de Vehı́culos/car_test.csv')\n",
    "validation = pd.read_csv('/Users/maxi/Downloads/Actuales/ML/ML-TP3/Data/3 - Evaluación de Aceptación de Vehı́culos/car_valid.csv')\n",
    "\n",
    "# one hot encoding\n",
    "train = one_hot_encoder(train, ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "test = one_hot_encoder(test, ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "validation = one_hot_encoder(validation, ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n",
    "                                          \n",
    "# target encoding   # unacc:0, acc: 1, good: 2, vgood: 3\n",
    "train['acceptability'] = train['acceptability'].map({'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3})\n",
    "test['acceptability'] = test['acceptability'].map({'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3})\n",
    "validation['acceptability'] = validation['acceptability'].map({'unacc': 0, 'acc': 1, 'good': 2, 'vgood': 3})\n",
    "\n",
    "# normalize all data\n",
    "# min_max = get_min_max(train)\n",
    "# train = min_max_normalize(train, min_max)\n",
    "# test = min_max_normalize(test, min_max)\n",
    "# validation = min_max_normalize(validation, min_max)\n",
    "\n",
    "# if any column has a missing value replace with cero\n",
    "# train = train.fillna(0)\n",
    "# test = test.fillna(0)\n",
    "# validation = validation.fillna(0)\n",
    "\n",
    "print(\"train shape: \", train.shape)\n",
    "print(\"\\ntest shape: \", test.shape)\n",
    "print(\"\\nvalidation shape: \", validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:\n",
      "acceptability\n",
      "0    847\n",
      "1    269\n",
      "2     48\n",
      "3     45\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify how many samples we have for each class in the training set\n",
    "print(\"train:\")\n",
    "print(train['acceptability'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classes are unbalanced. As a solution to this problem I try using Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train (after Oversampling):\n",
      "acceptability\n",
      "0    847\n",
      "1    269\n",
      "2     96\n",
      "3     90\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Oversampling: duplico todos los datos de las clases 2 y 3 del train\n",
    "class_2 = train[train['acceptability'] == 2]\n",
    "class_3 = train[train['acceptability'] == 3]\n",
    "\n",
    "double_class_2 = pd.concat([class_2], ignore_index=True)\n",
    "double_class_3 = pd.concat([class_3], ignore_index=True)\n",
    "\n",
    "train = pd.concat([train, double_class_2, double_class_3], ignore_index=True)\n",
    "\n",
    "print(\"train (after Oversampling):\")\n",
    "print(train['acceptability'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (1302, 21)\n",
      "y_train shape:  (1302,)\n",
      "X_test shape:  (260, 21)\n",
      "y_test shape:  (260,)\n",
      "X_validation shape:  (259, 21)\n",
      "y_validation shape:  (259,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acceptability</th>\n",
       "      <th>buying_high</th>\n",
       "      <th>buying_low</th>\n",
       "      <th>buying_med</th>\n",
       "      <th>buying_vhigh</th>\n",
       "      <th>maint_high</th>\n",
       "      <th>maint_low</th>\n",
       "      <th>maint_med</th>\n",
       "      <th>maint_vhigh</th>\n",
       "      <th>doors_2</th>\n",
       "      <th>doors_3</th>\n",
       "      <th>doors_4</th>\n",
       "      <th>doors_5more</th>\n",
       "      <th>persons_2</th>\n",
       "      <th>persons_4</th>\n",
       "      <th>persons_more</th>\n",
       "      <th>lug_boot_big</th>\n",
       "      <th>lug_boot_med</th>\n",
       "      <th>lug_boot_small</th>\n",
       "      <th>safety_high</th>\n",
       "      <th>safety_low</th>\n",
       "      <th>safety_med</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   acceptability  buying_high  buying_low  buying_med  buying_vhigh  \\\n",
       "0              0            0           0           1             0   \n",
       "1              0            0           0           1             0   \n",
       "2              0            0           0           0             1   \n",
       "3              3            0           1           0             0   \n",
       "4              0            0           1           0             0   \n",
       "\n",
       "   maint_high  maint_low  maint_med  maint_vhigh  doors_2  doors_3  doors_4  \\\n",
       "0           1          0          0            0        0        0        0   \n",
       "1           0          0          1            0        0        1        0   \n",
       "2           1          0          0            0        0        0        1   \n",
       "3           1          0          0            0        0        1        0   \n",
       "4           0          0          1            0        1        0        0   \n",
       "\n",
       "   doors_5more  persons_2  persons_4  persons_more  lug_boot_big  \\\n",
       "0            1          0          0             1             0   \n",
       "1            0          1          0             0             1   \n",
       "2            0          0          1             0             0   \n",
       "3            0          0          1             0             1   \n",
       "4            0          0          1             0             1   \n",
       "\n",
       "   lug_boot_med  lug_boot_small  safety_high  safety_low  safety_med  \n",
       "0             0               1            0           1           0  \n",
       "1             0               0            0           1           0  \n",
       "2             0               1            0           0           1  \n",
       "3             0               0            1           0           0  \n",
       "4             0               0            0           1           0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = train.drop(columns=['acceptability'])\n",
    "y_train = train['acceptability'].values\n",
    "\n",
    "X_test = test.drop(columns=['acceptability'])\n",
    "y_test = test['acceptability'].values\n",
    "\n",
    "X_validation = validation.drop(columns=['acceptability'])\n",
    "y_validation = validation['acceptability'].values\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"X_test shape: \", X_test.shape)\n",
    "print(\"y_test shape: \", y_test.shape)\n",
    "print(\"X_validation shape: \", X_validation.shape)\n",
    "print(\"y_validation shape: \", y_validation.shape)\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "train.head()\n",
    "test.head()\n",
    "validation.head()\n",
    "# X_train.head()\n",
    "# y_train\n",
    "\n",
    "# # show all rows\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8108108108108109\n",
      "F1 Score:  0.6588235294117647\n",
      "Precision:  0.717948717948718\n",
      "Recall:  0.6086956521739131\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(28, 170, 11, 18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNN(k=3)\n",
    "knn_model.fit(X_train, y_train)\n",
    "y_pred = knn_model.predict(X_validation)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred))\n",
    "print(\"F1 Score: \", f1(y_validation, y_pred))\n",
    "print(\"Precision: \", precision(y_validation, y_pred))\n",
    "print(\"Recall: \", recall(y_validation, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_validation, y_pred)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9845559845559846\n",
      "Precision:  0.9649122807017544\n",
      "Recall:  1.0\n",
      "F1 Score:  0.9821428571428572\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(55, 179, 2, 0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_model = DecisionTree(max_depth=10)\n",
    "dt_model.fit(X_train.values, y_train)\n",
    "y_pred = dt_model.predict(X_validation.values)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred))\n",
    "print(\"Precision: \", precision(y_validation, y_pred))\n",
    "print(\"Recall: \", recall(y_validation, y_pred))\n",
    "print(\"F1 Score: \", f1(y_validation, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_validation, y_pred)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:06<00:00, 14.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9806949806949807\n",
      "Precision:  0.9642857142857143\n",
      "Recall:  0.9818181818181818\n",
      "F1 Score:  0.972972972972973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(54, 179, 2, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = RandomForest(n_trees=100, max_depth=10)\n",
    "rf_model.fit(X_train.values, y_train)\n",
    "y_pred = rf_model.predict(X_validation.values)\n",
    "\n",
    "print(\"Accuracy: \", accuracy(y_validation, y_pred))\n",
    "print(\"Precision: \", precision(y_validation, y_pred))\n",
    "print(\"Recall: \", recall(y_validation, y_pred))\n",
    "print(\"F1 Score: \", f1(y_validation, y_pred))\n",
    "\n",
    "# confusion matrix\n",
    "conf_matrix = confusion_matrix(y_validation, y_pred)\n",
    "conf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "Accuracy:  0.8346153846153846\n",
      "Precision:  0.7804878048780488\n",
      "Recall:  0.64\n",
      "F1 Score:  0.7032967032967035\n",
      "confusion matrix: (32, 170, 9, 18)\n",
      "\n",
      "Decision Tree\n",
      "Accuracy:  0.9807692307692307\n",
      "Precision:  0.9354838709677419\n",
      "Recall:  1.0\n",
      "F1 Score:  0.9666666666666666\n",
      "confusion matrix: (58, 178, 4, 0)\n"
     ]
    }
   ],
   "source": [
    "# results on test set\n",
    "y_pred = knn_model.predict(X_test)\n",
    "print(\"KNN\")\n",
    "print(\"Accuracy: \", accuracy(y_test, y_pred))\n",
    "print(\"Precision: \", precision(y_test, y_pred))\n",
    "print(\"Recall: \", recall(y_test, y_pred))\n",
    "print(\"F1 Score: \", f1(y_test, y_pred))\n",
    "print(f\"confusion matrix: {confusion_matrix(y_test, y_pred)}\")\n",
    "\n",
    "y_pred = dt_model.predict(X_test.values)\n",
    "print(\"\\nDecision Tree\")\n",
    "print(\"Accuracy: \", accuracy(y_test, y_pred))\n",
    "print(\"Precision: \", precision(y_test, y_pred))\n",
    "print(\"Recall: \", recall(y_test, y_pred))\n",
    "print(\"F1 Score: \", f1(y_test, y_pred))\n",
    "print(f\"confusion matrix: {confusion_matrix(y_test, y_pred)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando RandomForest estoy obteniendo buenos resultados. Hago cross-validation para encontrar los mejores hiperparámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths = [5, 8, 10, 12, 15]\n",
    "forest_sizes = [10, 50, 100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 60.07it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m acc_train, acc_test \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depths\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforest_sizes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Downloads/Actuales/ML/ML-TP3/Problema 3/funciones_3.py:92\u001b[0m, in \u001b[0;36mcross_validation\u001b[0;34m(X, y, max_depths, forest_sizes, k, seed)\u001b[0m\n\u001b[1;32m     90\u001b[0m rf_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[1;32m     91\u001b[0m y_train_pred \u001b[38;5;241m=\u001b[39m rf_model\u001b[38;5;241m.\u001b[39mpredict(X_train)\n\u001b[0;32m---> 92\u001b[0m y_test_pred \u001b[38;5;241m=\u001b[39m \u001b[43mrf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m acc_train[i, j] \u001b[38;5;241m=\u001b[39m accuracy(y_train, y_train_pred)\n\u001b[1;32m     95\u001b[0m acc_test[i, j] \u001b[38;5;241m=\u001b[39m accuracy(y_test, y_test_pred)\n",
      "File \u001b[0;32m~/Downloads/Actuales/ML/ML-TP3/Problema 3/funciones_3.py:301\u001b[0m, in \u001b[0;36mRandomForest.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 301\u001b[0m     tree_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([tree\u001b[38;5;241m.\u001b[39mpredict(X) \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees])\n\u001b[1;32m    302\u001b[0m     tree_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mswapaxes(tree_preds, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    303\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_most_common_label(tree_pred) \u001b[38;5;28;01mfor\u001b[39;00m tree_pred \u001b[38;5;129;01min\u001b[39;00m tree_preds]\n",
      "File \u001b[0;32m~/Downloads/Actuales/ML/ML-TP3/Problema 3/funciones_3.py:301\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 301\u001b[0m     tree_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrees])\n\u001b[1;32m    302\u001b[0m     tree_preds \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mswapaxes(tree_preds, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    303\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_most_common_label(tree_pred) \u001b[38;5;28;01mfor\u001b[39;00m tree_pred \u001b[38;5;129;01min\u001b[39;00m tree_preds]\n",
      "File \u001b[0;32m~/Downloads/Actuales/ML/ML-TP3/Problema 3/funciones_3.py:198\u001b[0m, in \u001b[0;36mDecisionTree.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traverse_tree(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n",
      "File \u001b[0;32m~/Downloads/Actuales/ML/ML-TP3/Problema 3/funciones_3.py:198\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_traverse_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X])\n",
      "File \u001b[0;32m~/Downloads/Actuales/ML/ML-TP3/Problema 3/funciones_3.py:269\u001b[0m, in \u001b[0;36mDecisionTree._traverse_tree\u001b[0;34m(self, x, node)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mis_leaf_node():\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mthreshold:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traverse_tree(x, node\u001b[38;5;241m.\u001b[39mleft)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_traverse_tree(x, node\u001b[38;5;241m.\u001b[39mright)\n",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "acc_train, acc_test = cross_validation(X_train, y_train, max_depths, forest_sizes, k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
